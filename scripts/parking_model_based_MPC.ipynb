{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eeje4O8fviH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model-Based Reinforcement Learning\n",
    "\n",
    "## Principle\n",
    "We consider the optimal control problem of an MDP with a **known** reward function $R$ and subject to **unknown deterministic** dynamics $s_{t+1} = f(s_t, a_t)$:\n",
    "\n",
    "$$\\max_{(a_0,a_1,\\dotsc)} \\sum_{t=0}^\\infty \\gamma^t R(s_t,a_t)$$\n",
    "\n",
    "In **model-based reinforcement learning**, this problem is solved in **two steps**:\n",
    "1. **Model learning**:\n",
    "We learn a model of the dynamics $f_\\theta \\simeq f$ through regression on interaction data.\n",
    "2. **Planning**:\n",
    "We leverage the dynamics model $f_\\theta$ to compute the optimal trajectory $$\\max_{(a_0,a_1,\\dotsc)} \\sum_{t=0}^\\infty \\gamma^t R(\\hat{s}_t,a_t)$$ following the learnt dynamics $\\hat{s}_{t+1} = f_\\theta(\\hat{s}_t, a_t)$.\n",
    "\n",
    "(We can easily extend to unknown rewards and stochastic dynamics, but we consider the simpler case in this notebook for ease of presentation)\n",
    "\n",
    "\n",
    "## Motivation\n",
    "\n",
    "### Sparse rewards\n",
    "* In model-free reinforcement learning, we only obtain a reinforcement signal when encountering rewards. In environment with **sparse rewards**, the chance of obtaining a reward randomly is **negligible**, which prevents any learning.\n",
    "* However, even in the **absence of rewards** we still receive a **stream of state transition data**. We can exploit this data to learn about the task at hand.\n",
    "\n",
    "### Complexity of the policy/value vs dynamics:\n",
    "Is it easier to decide which action is best, or to predict what is going to happen?\n",
    "* Some problems can have **complex dynamics** but a **simple optimal policy or value function**. For instance, consider the problem of learning to swim. Predicting the movement requires understanding fluid dynamics and vortices while the optimal policy simply consists in moving the limbs in sync.\n",
    "* Conversely, other problems can have **simple dynamics** but **complex policies/value functions**. Think of the game of Go, its rules are simplistic (placing a stone merely changes the board state at this location) but the corresponding optimal policy is very complicated.\n",
    "\n",
    "Intuitively, model-free RL should be applied to the first category of problems and model-based RL to the second category.\n",
    "\n",
    "### Inductive bias\n",
    "Oftentimes, real-world problems exhibit a particular **structure**: for instance, any problem involving motion of physical objects will be **continuous**. It can also be **smooth**, **invariant** to translations, etc. This knowledge can then be incorporated in machine learning models to foster efficient learning. In contrast, there can often be **discontinuities** in the policy decisions or value function: e.g. think of a collision vs near-collision state.\n",
    "\n",
    "###  Sample efficiency\n",
    "Overall, it is generally recognized that model-based approaches tend to **learn faster** than model-free techniques (see e.g. [[Sutton, 1990]](http://papersdb.cs.ualberta.ca/~papersdb/uploaded_files/paper_p160-sutton.pdf.stjohn)).\n",
    "\n",
    "### Interpretability\n",
    "In real-world applications, we may want to know **how a policy will behave before actually executing it**, for instance for **safety-check** purposes. However, model-free reinforcement learning only recommends which action to take at current time without being able to predict its consequences. In order to obtain the trajectory, we have no choice but executing the policy. In stark contrast, model-based methods a more interpretable in the sense that we can probe the policy for its intended (and predicted) trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-oVNY_KTw6R"
   },
   "source": [
    "## Our challenge: Automated Parking System\n",
    "\n",
    "We consider the **parking-v0** task of the [highway-env](https://github.com/eleurent/highway-env) environment. It is a **goal-conditioned continuous control** task where an agent **drives a car** by controlling the gaz pedal and steering angle and must **park in a given location** with the appropriate heading.\n",
    "\n",
    "This MDP has several properties which justifies using model-based methods:\n",
    "* The policy/value is highly dependent on the goal which adds a significant level of complexity to a model-free learning process, whereas the dynamics are completely independent of the goal and hence can be simpler to learn.\n",
    "* In the context of an industrial application, we can reasonably expect for safety concerns that the planned trajectory is required to be known in advance, before execution.\n",
    "\n",
    "###  Warming up\n",
    "We start with a few useful installs and imports:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2Bu_Pqop0E7"
   },
   "source": [
    "We also define a simple helper function for visualization of episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install environment and visualization dependencies \n",
    "# !pip install highway-env\n",
    "\n",
    "# Environment\n",
    "# Force reload highway_env module\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Remove all highway_env related modules from cache\n",
    "modules_to_remove = [key for key in sys.modules.keys() if 'highway_env' in key]\n",
    "for module in modules_to_remove:\n",
    "    del sys.modules[module]\n",
    "    \n",
    "print(f\"ðŸ”„ Cleared {len(modules_to_remove)} cached modules\")\n",
    "\n",
    "# Now import fresh\n",
    "import highway_env\n",
    "import gymnasium as gym\n",
    "\n",
    "print(\"âœ… Fresh import complete!\")\n",
    "\n",
    "gym.register_envs(highway_env)\n",
    "\n",
    "# Models and computation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline\n",
    "\n",
    "# deleting old videos\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "folder_path = r\"/home/aayush_wsl/cs269_rl_parking/CS269-Parking/videos\"\n",
    "folder_path2 = r\"/home/aayush_wsl/cs269_rl_parking/CS269-Parking/scripts/videos\"\n",
    "if os.path.exists(folder_path):\n",
    "    shutil.rmtree(folder_path)\n",
    "    print(\"Video Folder deleted.\")\n",
    "else:\n",
    "    print(\"Video Folder does not exist.\")\n",
    "\n",
    "if os.path.exists(folder_path2):\n",
    "    shutil.rmtree(folder_path2)\n",
    "    print(\"Video Folder deleted.\")\n",
    "else:\n",
    "    print(\"Video Folder does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "so7yH4ucyB-3"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from tqdm.notebook import trange\n",
    "from tqdm import trange\n",
    "# !pip install tensorboardx gym pyvirtualdisplay\n",
    "# !apt-get install -y xvfb ffmpeg\n",
    "# !git clone https://github.com/Farama-Foundation/HighwayEnv.git 2> /dev/null\n",
    "sys.path.insert(0, '/home/aayush_wsl/cs269_rl_parking/CS269-Parking/scripts')\n",
    "from utils import record_videos, show_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify import location\n",
    "print(\"ðŸ” Importing from:\", highway_env.__file__)\n",
    "print()\n",
    "\n",
    "# Create environment WITHOUT passing config\n",
    "env = gym.make(\"parking-v0\")\n",
    "config = env.unwrapped.config\n",
    "\n",
    "# Check the configuration\n",
    "print(\"ðŸ“‹ Current Default Configuration:\")\n",
    "print(f\"  Duration: {config['duration']}\")\n",
    "print(f\"  Policy frequency: {config['policy_frequency']}\")\n",
    "print(f\"  Vehicles count: {config['vehicles_count']}\")\n",
    "print(f\"  Steering range: {config['steering_range']}\")\n",
    "print(f\"  Reward weights: {config['reward_weights']}\")\n",
    "print(f\"  Success goal reward: {config['success_goal_reward']}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFtBY6JSqPFa"
   },
   "source": [
    "### Let's try it!\n",
    "\n",
    "Make the environment, and run an episode with random actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full configuration dictionary for parking-v0 environment\n",
    "parking_config = {\n",
    "    # Observation configuration\n",
    "    \"observation\": {\n",
    "        \"type\": \"KinematicsGoal\",\n",
    "        \"features\": [\"x\", \"y\", \"vx\", \"vy\", \"cos_h\", \"sin_h\"],\n",
    "        \"scales\": [100, 100, 5, 5, 1, 1],\n",
    "        \"normalize\": False,\n",
    "    },\n",
    "    \n",
    "    # Action configuration\n",
    "    \"action\": {\n",
    "        \"type\": \"ContinuousAction\",\n",
    "        # \"acceleration_range\": (-1, 1),\n",
    "        # \"speed_range\": (-1.5, 1.5),\n",
    "    },\n",
    "    \n",
    "    # Reward parameters\n",
    "    \"reward_weights\": [1, 1, 0.1, 0.1, 1, 1],  # Weights for [x, y, vx, vy, cos_h, sin_h]\n",
    "    \"success_goal_reward\": 0.12,\n",
    "    \"collision_reward\": -5,\n",
    "    \n",
    "    # Vehicle control parameters\n",
    "    \"steering_range\": np.deg2rad(60),  # Maximum steering angle in radians\n",
    "    \n",
    "    # Simulation parameters\n",
    "    \"simulation_frequency\": 15,  # Hz\n",
    "    \"policy_frequency\": 5,       # Hz\n",
    "    \"duration\": 200,             # Maximum episode duration in steps\n",
    "    \n",
    "    # Rendering parameters\n",
    "    \"screen_width\": 600,\n",
    "    \"screen_height\": 300,\n",
    "    \"centering_position\": [0.5, 0.5],\n",
    "    \"scaling\": 7,\n",
    "    \"show_trajectories\": True,\n",
    "    \n",
    "    # Environment setup\n",
    "    \"controlled_vehicles\": 1,    # Number of vehicles to control\n",
    "    \"vehicles_count\": 24,         # Number of parked vehicles (obstacles)\n",
    "    \"add_walls\": True,           # Whether to add boundary walls\n",
    "\n",
    "    # Additional parameters from AbstractEnv\n",
    "    \"offscreen_rendering\": False,\n",
    "    \"manual_control\": False,\n",
    "    \"real_time_rendering\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"parking-v0\", render_mode=\"rgb_array\", config=parking_config)\n",
    "config = env.unwrapped.config\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸš— PARKING ENVIRONMENT CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ“Š OBSERVATION SETTINGS:\")\n",
    "print(f\"  Type: {config['observation']['type']}\")\n",
    "print(f\"  Features: {config['observation']['features']}\")\n",
    "print(f\"  Scales: {config['observation']['scales']}\")\n",
    "print(f\"  Normalize: {config['observation']['normalize']}\")\n",
    "\n",
    "print(\"\\nðŸŽ® ACTION SETTINGS:\")\n",
    "print(f\"  Type: {config['action']['type']}\")\n",
    "# print(f\"  Acceleration range: {config['action']['acceleration_range']}\")\n",
    "# print(f\"  Speed range: {config['action']['speed_range']}\")\n",
    "# print(f\"  Steering range: {config['steering_range']:.4f} rad ({np.rad2deg(config['steering_range']):.1f}Â°)\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ REWARD SETTINGS:\")\n",
    "print(f\"  Reward weights [x, y, vx, vy, cos_h, sin_h]: {config['reward_weights']}\")\n",
    "print(f\"  Success goal threshold: {config['success_goal_reward']}\")\n",
    "print(f\"  Collision penalty: {config['collision_reward']}\")\n",
    "\n",
    "print(\"\\nâ±ï¸ SIMULATION SETTINGS:\")\n",
    "print(f\"  Simulation frequency: {config['simulation_frequency']} Hz\")\n",
    "print(f\"  Policy frequency: {config['policy_frequency']} Hz\")\n",
    "print(f\"  Max episode duration: {config['duration']} steps\")\n",
    "print(f\"  Real-time duration: {config['duration'] / config['policy_frequency']:.1f} seconds\")\n",
    "\n",
    "print(\"\\nðŸŽ¨ RENDERING SETTINGS:\")\n",
    "print(f\"  Screen size: {config['screen_width']}x{config['screen_height']}\")\n",
    "print(f\"  Centering position: {config['centering_position']}\")\n",
    "print(f\"  Scaling: {config['scaling']}\")\n",
    "\n",
    "print(\"\\nðŸš™ ENVIRONMENT SETUP:\")\n",
    "print(f\"  Controlled vehicles: {config['controlled_vehicles']}\")\n",
    "print(f\"  Parked vehicles (obstacles): {config['vehicles_count']}\")\n",
    "print(f\"  Boundary walls: {config['add_walls']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKZt9Cb1rJ6n"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"parking-v0\", render_mode=\"rgb_array\", config = parking_config)\n",
    "env = record_videos(env)\n",
    "env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "env.close()\n",
    "show_videos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewG5f_essAS0"
   },
   "source": [
    "The environment is a `GoalEnv`, which means the agent receives a dictionary containing both the current `observation` and the `desired_goal` that conditions its policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIC98mGhr7v6"
   },
   "outputs": [],
   "source": [
    "print(\"Observation format:\", obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voagCILztJ3J"
   },
   "source": [
    "There is also an `achieved_goal` that won't be useful here (it only serves when the state and goal spaces are different, as a projection from the observation to the goal space).\n",
    "\n",
    "Alright! We are now ready to apply the model-based reinforcement learning paradigm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2PuVAvyfvib",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Experience collection\n",
    "First, we randomly interact with the environment to produce a batch of experiences \n",
    "\n",
    "$$D = \\{s_t, a_t, s_{t+1}\\}_{t\\in[1,N]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tvUYSL7sfvie",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Transition = namedtuple('Transition', ['state', 'action', 'next_state'])\n",
    "\n",
    "# def collect_interaction_data(env, size=5000, action_repeat=1):\n",
    "#     data, done = [], True\n",
    "#     for _ in trange(size, desc=\"Collecting interaction data\"):\n",
    "#         action = env.action_space.sample()\n",
    "#         for _ in range(action_repeat):\n",
    "#             if done:\n",
    "#               previous_obs, info = env.reset()\n",
    "#             obs, reward, done, truncated, info = env.step(action)\n",
    "#             data.append(Transition(torch.Tensor(previous_obs[\"observation\"]),\n",
    "#                                    torch.Tensor(action),\n",
    "#                                    torch.Tensor(obs[\"observation\"])))\n",
    "#             previous_obs = obs\n",
    "#     return data\n",
    "\n",
    "Transition = namedtuple('Transition', ['state', 'action', 'next_state'])\n",
    "\n",
    "def collect_interaction_data(env, size=10000, action_repeat=1):\n",
    "    data, done = [], True\n",
    "    episode_count = 0\n",
    "    episode_lengths = []\n",
    "    current_episode_length = 0\n",
    "    \n",
    "    for _ in trange(size, desc=\"Collecting interaction data\"):\n",
    "        action = env.action_space.sample()\n",
    "        for _ in range(action_repeat):\n",
    "            if done:\n",
    "                if episode_count > 0:\n",
    "                    episode_lengths.append(current_episode_length)\n",
    "                previous_obs, info = env.reset()\n",
    "                episode_count += 1\n",
    "                current_episode_length = 0\n",
    "                \n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            current_episode_length += 1\n",
    "            data.append(Transition(torch.Tensor(previous_obs[\"observation\"]),\n",
    "                                   torch.Tensor(action),\n",
    "                                   torch.Tensor(obs[\"observation\"])))\n",
    "            previous_obs = obs\n",
    "            done = done or truncated  # Combine done flags\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nðŸ“Š Collection Statistics:\")\n",
    "    print(f\"  Total transitions: {len(data)}\")\n",
    "    print(f\"  Number of episodes: {episode_count}\")\n",
    "    print(f\"  Average episode length: {np.mean(episode_lengths):.1f} steps\")\n",
    "    print(f\"  Min/Max episode length: {min(episode_lengths)}/{max(episode_lengths)} steps\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "env = gym.make(\"parking-v0\", render_mode=\"rgb_array\", config=parking_config)\n",
    "print(env.unwrapped.config['action'])\n",
    "print(env.unwrapped.config)\n",
    "data = collect_interaction_data(env)\n",
    "print(\"Sample transition:\", data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data in a pickle file within the scripts folder with a data time stamp\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "# Get current date and time as a string\n",
    "data_timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define the filename with the timestamp\n",
    "filename = f\"interaction_data_{data_timestamp}.pkl\" \n",
    "\n",
    "# Save the data to the file\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Th1JezEfvir"
   },
   "source": [
    "## Build a dynamics model\n",
    "\n",
    "We now design a model to represent the system dynamics. We choose  a **structured model** inspired from *Linear Time-Invariant (LTI) systems* \n",
    "\n",
    "$$\\dot{x} = f_\\theta(x, u) = A_\\theta(x, u)x + B_\\theta(x, u)u$$\n",
    "\n",
    "where the $(x, u)$ notation comes from the Control Theory community and stands for the state and action $(s,a)$. Intuitively, we learn at each point $(x_t, u_t)$ the **linearization** of the true dynamics $f$ with respect to $(x, u)$.\n",
    "\n",
    "We parametrize $A_\\theta$ and $B_\\theta$ as two fully-connected networks with one hidden layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7Gl2kKJfviu",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DynamicsModel(nn.Module):\n",
    "    STATE_X = 0\n",
    "    STATE_Y = 1\n",
    "\n",
    "    def __init__(self, state_size, action_size, hidden_size, dt):\n",
    "        super().__init__()\n",
    "        self.state_size, self.action_size, self.dt = state_size, action_size, dt\n",
    "        A_size, B_size = state_size * state_size, state_size * action_size\n",
    "        self.A1 = nn.Linear(state_size + action_size, hidden_size)\n",
    "        self.A2 = nn.Linear(hidden_size, A_size)\n",
    "        self.B1 = nn.Linear(state_size + action_size, hidden_size)\n",
    "        self.B2 = nn.Linear(hidden_size, B_size)\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        \"\"\"\n",
    "            Predict x_{t+1} = f(x_t, u_t)\n",
    "        :param x: a batch of states\n",
    "        :param u: a batch of actions\n",
    "        \"\"\"\n",
    "        xu = torch.cat((x, u), -1)\n",
    "        xu[:, self.STATE_X:self.STATE_Y+1] = 0  # Remove dependency in (x,y)\n",
    "        A = self.A2(F.relu(self.A1(xu)))\n",
    "        A = torch.reshape(A, (x.shape[0], self.state_size, self.state_size))\n",
    "        B = self.B2(F.relu(self.B1(xu)))\n",
    "        B = torch.reshape(B, (x.shape[0], self.state_size, self.action_size))\n",
    "        dx = A @ x.unsqueeze(-1) + B @ u.unsqueeze(-1)\n",
    "        return x + dx.squeeze()*self.dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFsgc7gffvi0"
   },
   "source": [
    "## Fit the model on data\n",
    "We can now train our model $f_\\theta$ in a supervised fashion to minimize an MSE loss $L^2(f_\\theta; D)$ over our experience batch $D$ by stochastic gradient descent:\n",
    "\n",
    "$$L^2(f_\\theta; D) = \\frac{1}{|D|}\\sum_{s_t,a_t,s_{t+1}\\in D}||s_{t+1}- f_\\theta(s_t, a_t)||^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwCDLD1wfvi2",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ===== DATA SPLIT =====\n",
    "# Load data from the pickle file\n",
    "import pickle\n",
    "filename = r'/home/aayush_wsl/cs269_rl_parking/CS269-Parking/scripts/interaction_data_20251128_034835.pkl'\n",
    "with open(filename, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "dynamics = DynamicsModel(state_size=env.observation_space.spaces[\"observation\"].shape[0],\n",
    "                         action_size=env.action_space.shape[0],\n",
    "                         hidden_size=96,  # Reduced from 96\n",
    "                         dt=1/env.unwrapped.config[\"policy_frequency\"]\n",
    "                         ) \n",
    "print(\"Forward initial model on a sample transition:\",\n",
    "      dynamics(data[0].state.unsqueeze(0), data[0].action.unsqueeze(0)).detach())\n",
    "\n",
    "#optimizer = torch.optim.Adam(dynamics.parameters(), lr=0.01)\n",
    "# ===== OPTIMIZER & SCHEDULER =====\n",
    "optimizer = torch.optim.Adam(\n",
    "    dynamics.parameters(), \n",
    "    lr=0.005,              # Reduced from 0.01 (â†“50%)\n",
    "    weight_decay=1e-5      # L2 regularization to prevent overfitting\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,            # Halve LR when plateau\n",
    "    patience=50,           # Wait 50 epochs\n",
    "    min_lr=1e-5,           # Don't go below this\n",
    ")\n",
    "\n",
    "# Split dataset into training and validation\n",
    "# # train_ratio = 0.7\n",
    "# train_data, validation_data = data[:int(train_ratio * len(data))], data[int(train_ratio * len(data)):]\n",
    "train_ratio = 0.7         # Increased from 0.7 (more training data)\n",
    "train_data = data[:int(train_ratio * len(data))]      # 4000 samples\n",
    "validation_data = data[int(train_ratio * len(data)):] # 1000 samples\n",
    "\n",
    "def compute_loss(model, data_t, loss_func = torch.nn.MSELoss()):\n",
    "    states, actions, next_states = data_t\n",
    "    predictions = model(states, actions)\n",
    "    return loss_func(predictions, next_states)\n",
    "\n",
    "def transpose_batch(batch):\n",
    "    return Transition(*map(torch.stack, zip(*batch)))\n",
    "\n",
    "# ===== TRAINING SETTINGS =====\n",
    "early_stopping_patience = 80\n",
    "def train(model, train_data, validation_data, epochs=1500):\n",
    "    train_data_t = transpose_batch(train_data)\n",
    "    validation_data_t = transpose_batch(validation_data)\n",
    "    losses = []\n",
    "\n",
    "    # ===== New code =====\n",
    "    best_val_loss = float('inf') \n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in trange(epochs, desc=\"Train dynamics\"):\n",
    "        # Forward pass\n",
    "        loss = compute_loss(model, train_data_t)\n",
    "        validation_loss = compute_loss(model, validation_data_t)\n",
    "        \n",
    "        # Store losses\n",
    "        losses.append([loss.item(), validation_loss.item()])\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (prevents exploding gradients)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update learning rate based on validation loss\n",
    "        scheduler.step(validation_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if validation_loss < best_val_loss:\n",
    "            best_val_loss = validation_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= early_stopping_patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "            print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "        \n",
    "        # Print progress every 100 epochs\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"\\nEpoch {epoch+1}: train={loss.item():.6f}, \"\n",
    "                  f\"val={validation_loss.item():.6f}, lr={current_lr:.6f}\")\n",
    "    \n",
    "    # Plot losses\n",
    "    losses = np.array(losses)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses[:, 0], label='train', alpha=0.8)\n",
    "    plt.plot(losses[:, 1], label='validation', alpha=0.8)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"loss (MSE)\")\n",
    "    plt.title(\"Training Progress\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Train the model\n",
    "losses = train(dynamics, train_data, validation_data, epochs=600)\n",
    "    # ===== End of new code =====\n",
    "\n",
    "# ==== Legacy code =====\n",
    "#     for epoch in trange(epochs, desc=\"Train dynamics\"):\n",
    "#         # Compute loss gradient and step optimizer\n",
    "#         loss = compute_loss(model, train_data_t)\n",
    "#         validation_loss = compute_loss(model, validation_data_t)\n",
    "#         losses[epoch] = [loss.detach().numpy(), validation_loss.detach().numpy()]\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     # Plot losses\n",
    "#     plt.plot(losses)\n",
    "#     plt.yscale(\"log\")\n",
    "#     plt.xlabel(\"epochs\")\n",
    "#     plt.ylabel(\"loss\")\n",
    "#     plt.legend([\"train\", \"validation\"])\n",
    "#     plt.show()\n",
    "\n",
    "# train(dynamics, data, validation_data)\n",
    "# ==== End of legacy code ====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXBODCuYfvi_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Visualize trained dynamics\n",
    "\n",
    "In order to qualitatively evaluate our model, we can choose some values of steering angle *(right, center, left)* and acceleration *(slow, fast)* in order to predict and visualize the corresponding trajectories from an initial state.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMPA55bCfvjB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#import the best dynamics model\n",
    "import pickle\n",
    "filename = r'/Users/anmol/githubRepos/CS269-Parking/scripts/dynamics_model_20251128_050826_best.pkl'\n",
    "with open(filename, 'rb') as f:\n",
    "    dynamics = pickle.load(f)\n",
    "\n",
    "def predict_trajectory(state, actions, model, action_repeat=1):\n",
    "    states = []\n",
    "    for action in actions:\n",
    "        for _ in range(action_repeat):\n",
    "            state = model(state, action)\n",
    "            states.append(state)\n",
    "    return torch.stack(states, dim=0)\n",
    "\n",
    "def plot_trajectory(states, color):\n",
    "    scales = np.array(env.unwrapped.config[\"observation\"][\"scales\"])\n",
    "    states = np.clip(states.squeeze(1).detach().numpy() * scales, -100, 100)\n",
    "    plt.plot(states[:, 0], states[:, 1], color=color, marker='.')\n",
    "    plt.arrow(states[-1,0], states[-1,1], states[-1,4]*1, states[-1,5]*1, color=color)\n",
    "\n",
    "def visualize_trajectories(model, state, horizon=15):\n",
    "    plt.cla()\n",
    "    # Draw a car\n",
    "    plt.plot(state.numpy()[0]+2.5*np.array([-1, -1, 1, 1, -1]),\n",
    "             state.numpy()[1]+1.0*np.array([-1, 1, 1, -1, -1]), 'k')\n",
    "    # Draw trajectories\n",
    "    state = state.unsqueeze(0)\n",
    "    colors = iter(plt.get_cmap(\"tab20\").colors)\n",
    "    # Generate commands\n",
    "    for steering in np.linspace(-0.5, 0.5, 3):\n",
    "        for acceleration in np.linspace(0.8, 0.4, 2):\n",
    "            actions = torch.Tensor([acceleration, steering]).view(1,1,-1)\n",
    "            # Predict trajectories\n",
    "            states = predict_trajectory(state, actions, model, action_repeat=horizon)\n",
    "            plot_trajectory(states, color=next(colors))\n",
    "    plt.axis(\"equal\")\n",
    "    plt.show()\n",
    "    \n",
    "visualize_trajectories(dynamics, state=torch.Tensor([0, 0, 0, 0, 1, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOa0j1_muNXi"
   },
   "source": [
    "## Reward model\n",
    "We assume that the reward $R(s,a)$ is known (chosen by the system designer), and takes the form of a **weighted L1-norm** between the state and the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRubRv9buNXj"
   },
   "outputs": [],
   "source": [
    "def reward_model(states, goal, gamma=None, terminal_weight=1.0):\n",
    "    \"\"\"\n",
    "        The reward is a weighted L1-norm between the state and a goal\n",
    "    :param Tensor states: a batch of states. shape: [batch_size, state_size].\n",
    "    :param Tensor goal: a goal state. shape: [state_size].\n",
    "    :param float gamma: a discount factor\n",
    "    \"\"\"\n",
    "    goal = goal.expand(states.shape)\n",
    "    reward_weigths = torch.Tensor(env.unwrapped.config[\"reward_weights\"])\n",
    "    rewards = -torch.pow(torch.norm((states-goal)*reward_weigths, p=1, dim=-1), 0.5)\n",
    "    if gamma:\n",
    "        time = torch.arange(rewards.shape[0], dtype=torch.float).unsqueeze(-1).expand(rewards.shape)\n",
    "        rewards *= torch.pow(gamma, time)\n",
    "\n",
    "    # NEW: Apply extra weight to terminal state (last timestep)\n",
    "    if terminal_weight != 1.0:\n",
    "        terminal_mask = torch.zeros_like(rewards)\n",
    "        terminal_mask[-1] = terminal_weight - 1.0  # Extra weight on last timestep\n",
    "        rewards = rewards * (1.0 + terminal_mask)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "obs, info = env.reset()\n",
    "print(\"Reward of a sample transition:\", reward_model(torch.Tensor(obs[\"observation\"]).unsqueeze(0),\n",
    "                                                     torch.Tensor(obs[\"desired_goal\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5D6W4p7fvjI"
   },
   "source": [
    "## Leverage dynamics model for planning\n",
    "\n",
    "We now use the learnt dynamics model $f_\\theta$ for planning.\n",
    "In order to solve the optimal control problem, we use a sampling-based optimization algorithm: the **Cross-Entropy Method** (`CEM`). It is an optimization algorithm applicable to problems that are both **combinatorial** and **continuous**, which is our case: find the best performing sequence of actions.\n",
    "\n",
    "This method approximates the optimal importance sampling estimator by repeating two phases:\n",
    "1. **Draw samples** from a probability distribution. We use Gaussian distributions over sequences of actions.\n",
    "2. Minimize the **cross-entropy** between this distribution and a **target distribution** to produce a better sample in the next iteration. We define this target distribution by selecting the top-k performing sampled sequences.\n",
    "\n",
    "![Credits to Olivier Sigaud](https://github.com/yfletberliac/rlss2019-hands-on/blob/master/imgs/cem.png?raw=1)\n",
    "\n",
    "Note that as we have a local linear dynamics model, we could instead choose an `Iterative LQR` planner which would be more efficient. We prefer `CEM` in this educational setting for its simplicity and generality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzPKYg23fvjL",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cem_planner(state, goal, action_size, horizon=12, population=200, selection=20, iterations=8):\n",
    "    state = state.expand(population, -1)\n",
    "    action_mean = torch.zeros(horizon, 1, action_size)\n",
    "    action_std = torch.ones(horizon, 1, action_size)\n",
    "    for _ in range(iterations):\n",
    "        # 1. Draw sample sequences of actions from a normal distribution\n",
    "        actions = torch.normal(mean=action_mean.repeat(1, population, 1), std=action_std.repeat(1, population, 1))\n",
    "        actions = torch.clamp(actions, min=env.action_space.low.min(), max=env.action_space.high.max())\n",
    "        states = predict_trajectory(state, actions, dynamics, action_repeat=1)\n",
    "        # 2. Fit the distribution to the top-k performing sequences\n",
    "        returns = reward_model(states, goal, gamma=None, terminal_weight = 2.5).sum(dim=0)\n",
    "        _, best = returns.topk(selection, largest=True, sorted=False)\n",
    "        best_actions = actions[:, best, :]\n",
    "        action_mean = best_actions.mean(dim=1, keepdim=True)\n",
    "        action_std = best_actions.std(dim=1, unbiased=False, keepdim=True)\n",
    "    return action_mean[0].squeeze(dim=0)\n",
    "  \n",
    "print(torch.Tensor(obs['desired_goal'])) \n",
    "# Run the planner on a sample transition\n",
    "action = cem_planner(torch.Tensor(obs[\"observation\"]),\n",
    "                     torch.Tensor(obs[\"desired_goal\"]),\n",
    "                     env.action_space.shape[0])\n",
    "print(\"Planned action:\", action)\n",
    "\n",
    "# Implementing Model Predictive Control with the dynamics model trained above to successfully do parallel parking\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPC implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "def mpc_planner(\n",
    "    state, \n",
    "    goal, \n",
    "    action_size, \n",
    "    dynamics_model,\n",
    "    horizon=12,\n",
    "    num_iterations=50,\n",
    "    learning_rate=0.1,\n",
    "    action_repeat=1,\n",
    "    terminal_weight=2.5,\n",
    "    verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Model Predictive Control planner using gradient-based optimization.\n",
    "    \n",
    "    Args:\n",
    "        state: Current state tensor [state_size]\n",
    "        goal: Goal state tensor [state_size]\n",
    "        action_size: Dimension of action space (2: acceleration, steering)\n",
    "        dynamics_model: Trained neural network dynamics model\n",
    "        horizon: Planning horizon (number of timesteps)\n",
    "        num_iterations: Number of optimization iterations\n",
    "        learning_rate: Learning rate for Adam optimizer\n",
    "        action_repeat: Number of times each action is repeated\n",
    "        terminal_weight: Extra weight on terminal state cost\n",
    "        verbose: Print optimization progress\n",
    "    \n",
    "    Returns:\n",
    "        Optimized first action [action_size]\n",
    "    \"\"\"\n",
    "    device = state.device\n",
    "    \n",
    "    # ===== STEP 1: Initialize action sequence =====\n",
    "    # Start with small random actions near zero (warm start)\n",
    "    action_sequence = torch.randn(\n",
    "        horizon, action_size, \n",
    "        device=device\n",
    "    ) * 0.1\n",
    "    action_sequence.requires_grad_(True) \n",
    "    # ===== STEP 2: Setup optimizer =====\n",
    "    optimizer = Adam([action_sequence], lr=learning_rate)\n",
    "    \n",
    "    # Action bounds from environment\n",
    "    action_min = torch.tensor(\n",
    "        env.action_space.low, \n",
    "        device=device, \n",
    "        dtype=torch.float32\n",
    "    )\n",
    "    action_max = torch.tensor(\n",
    "        env.action_space.high, \n",
    "        device=device, \n",
    "        dtype=torch.float32\n",
    "    )\n",
    "    \n",
    "    best_cost = float('inf')\n",
    "    best_action_sequence = None\n",
    "    \n",
    "    # ===== STEP 3: Optimization loop =====\n",
    "    for iteration in range(num_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Clamp actions to valid range\n",
    "        with torch.no_grad():\n",
    "            action_sequence.data = torch.clamp(\n",
    "                action_sequence.data, \n",
    "                action_min, \n",
    "                action_max\n",
    "            )\n",
    "        \n",
    "        # ===== STEP 4: Predict trajectory using dynamics model =====\n",
    "        current_state = state.unsqueeze(0)  # [1, state_size]\n",
    "        predicted_states = [current_state]\n",
    "        \n",
    "        for t in range(horizon):\n",
    "            action_t = action_sequence[t].unsqueeze(0)  # [1, action_size]\n",
    "            \n",
    "            # Predict next state using learned dynamics\n",
    "            next_state = dynamics_model(current_state, action_t)\n",
    "            predicted_states.append(next_state)\n",
    "            current_state = next_state\n",
    "        \n",
    "        # Stack all predicted states [horizon+1, state_size]\n",
    "        predicted_states = torch.cat(predicted_states, dim=0)\n",
    "        \n",
    "        # ===== STEP 5: Compute cost function =====\n",
    "        cost = compute_mpc_cost(\n",
    "            predicted_states,\n",
    "            goal,\n",
    "            action_sequence,\n",
    "            terminal_weight=terminal_weight\n",
    "        )\n",
    "        \n",
    "        # Track best solution\n",
    "        if cost.item() < best_cost:\n",
    "            best_cost = cost.item()\n",
    "            best_action_sequence = action_sequence.data.clone()\n",
    "        \n",
    "        # ===== STEP 6: Backpropagation and optimization step =====\n",
    "        cost.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_([action_sequence], max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if verbose and (iteration % 10 == 0 or iteration == num_iterations - 1):\n",
    "            print(f\"Iteration {iteration:3d}: Cost = {cost.item():.6f}\")\n",
    "    \n",
    "    # ===== STEP 7: Return first action from best sequence =====\n",
    "    return best_action_sequence[0]\n",
    "\n",
    "\n",
    "def compute_mpc_cost(states, goal, actions, terminal_weight=2.5):\n",
    "    \"\"\"\n",
    "    Compute the total cost for MPC optimization.\n",
    "    \n",
    "    Cost = State tracking cost + Terminal cost + Action regularization\n",
    "    \n",
    "    Args:\n",
    "        states: Predicted state trajectory [horizon+1, state_size]\n",
    "        goal: Goal state [state_size]\n",
    "        actions: Action sequence [horizon, action_size]\n",
    "        terminal_weight: Extra weight on terminal state\n",
    "    \n",
    "    Returns:\n",
    "        Total cost (scalar)\n",
    "    \"\"\"\n",
    "    # ===== 1. State Tracking Cost =====\n",
    "    # Use same reward function as CEM planner (negated = cost)\n",
    "    scales = torch.Tensor(env.unwrapped.config[\"observation\"][\"scales\"]) \n",
    "    reward_weights = torch.Tensor(env.unwrapped.config[\"reward_weights\"])\n",
    "    \n",
    "    # Expand goal to match states shape\n",
    "    goal_expanded = goal.unsqueeze(0).expand(states.shape[0], -1)\n",
    "    \n",
    "    # âœ… CORRECTED: Unscale to physical space, then apply weights\n",
    "    # This ensures we're computing actual physical distance errors\n",
    "    state_errors_physical = (states - goal_expanded) * scales  # Convert to physical units\n",
    "    weighted_errors = state_errors_physical * reward_weights    # Apply reward weights\n",
    "    state_cost = torch.pow(torch.norm(weighted_errors, p=1, dim=-1), 0.5)\n",
    "    \n",
    "    # ===== 2. Terminal Cost (extra weight on final state) =====\n",
    "    # Emphasize reaching goal at end of horizon\n",
    "    terminal_cost = state_cost[-1] * (terminal_weight - 1.0)\n",
    "    state_cost_total = state_cost.sum() + terminal_cost\n",
    "    \n",
    "    # ===== 3. Action Regularization =====\n",
    "    # Penalize large actions for smoother control\n",
    "    action_cost = 0.005 * torch.sum(actions ** 2)\n",
    "    \n",
    "    # Penalize large changes in actions (smooth trajectory)\n",
    "    action_smoothness_cost = 0.02 * torch.sum((actions[1:] - actions[:-1]) ** 2)\n",
    "    \n",
    "    # ===== 4. Velocity Penalty (for final state parking) =====\n",
    "    # States are [x, y, vx, vy, cos_h, sin_h], so indices 2,3 are velocities\n",
    "    final_velocity = states[-1, 2:4]  # [vx, vy] at final timestep\n",
    "    velocity_cost = 2.0 * torch.sum(final_velocity ** 2)  # âœ… NEW: Penalize non-zero final velocity\n",
    "\n",
    "    # ===== 4. Total Cost =====\n",
    "    total_cost = state_cost_total + action_cost + action_smoothness_cost + velocity_cost\n",
    "    \n",
    "    return total_cost\n",
    "\n",
    "\n",
    "def predict_trajectory(state, actions, dynamics_model, action_repeat=1):\n",
    "    \"\"\"\n",
    "    Predict state trajectory given action sequence.\n",
    "    \n",
    "    Args:\n",
    "        state: Initial state [batch_size, state_size] or [state_size]\n",
    "        actions: Action sequence [horizon, batch_size, action_size] or [horizon, action_size]\n",
    "        dynamics_model: Trained neural network model\n",
    "        action_repeat: Number of times to repeat each action\n",
    "    \n",
    "    Returns:\n",
    "        Predicted states [horizon, batch_size, state_size]\n",
    "    \"\"\"\n",
    "    if state.dim() == 1:\n",
    "        state = state.unsqueeze(0)\n",
    "    \n",
    "    states = []\n",
    "    current_state = state\n",
    "    \n",
    "    for t in range(actions.shape[0]):\n",
    "        if actions.dim() == 2:\n",
    "            action_t = actions[t].unsqueeze(0)\n",
    "        else:\n",
    "            action_t = actions[t]\n",
    "        \n",
    "        # Apply action for action_repeat steps\n",
    "        for _ in range(action_repeat):\n",
    "            current_state = dynamics_model(current_state, action_t)\n",
    "        \n",
    "        states.append(current_state)\n",
    "    \n",
    "    return torch.stack(states, dim=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative MPC + BFGS\n",
    "def mpc_planner_lbfgs(\n",
    "    state, \n",
    "    goal, \n",
    "    action_size, \n",
    "    dynamics_model,\n",
    "    horizon=12,\n",
    "    max_iterations=20,\n",
    "    action_repeat=1,\n",
    "    terminal_weight=2.5,\n",
    "    verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    MPC planner using L-BFGS optimizer (second-order method).\n",
    "    Often converges faster than Adam but requires more memory.\n",
    "    \"\"\"\n",
    "    device = state.device\n",
    "    \n",
    "    # Initialize action sequence\n",
    "    # Fix: Create tensor first, THEN make it require gradients\n",
    "    action_sequence = torch.randn(\n",
    "        horizon, action_size, \n",
    "        device=device\n",
    "    ) * 0.1  # Scale to small values\n",
    "    \n",
    "    # Now make it a leaf tensor that requires gradients\n",
    "    action_sequence.requires_grad_(True)  # âœ… This makes it optimizable!\n",
    "    \n",
    "    # Setup L-BFGS optimizer\n",
    "    optimizer = torch.optim.LBFGS(\n",
    "        [action_sequence],\n",
    "        lr=1.0,\n",
    "        max_iter=20,\n",
    "        max_eval=25,\n",
    "        tolerance_grad=1e-7,\n",
    "        tolerance_change=1e-9,\n",
    "        history_size=10,\n",
    "        line_search_fn='strong_wolfe'\n",
    "    )\n",
    "    \n",
    "    action_min = torch.tensor(env.action_space.low, device=device, dtype=torch.float32)\n",
    "    action_max = torch.tensor(env.action_space.high, device=device, dtype=torch.float32)\n",
    "    \n",
    "    iteration_count = [0]\n",
    "    \n",
    "    def closure():\n",
    "        \"\"\"Closure function required by L-BFGS\"\"\"\n",
    "        iteration_count[0] += 1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Clamp actions\n",
    "        with torch.no_grad():\n",
    "            action_sequence.data = torch.clamp(\n",
    "                action_sequence.data, \n",
    "                action_min, \n",
    "                action_max\n",
    "            )\n",
    "        \n",
    "        # Predict trajectory\n",
    "        current_state = state.unsqueeze(0)\n",
    "        predicted_states = [current_state]\n",
    "        \n",
    "        for t in range(horizon):\n",
    "            action_t = action_sequence[t].unsqueeze(0)\n",
    "            next_state = dynamics_model(current_state, action_t)\n",
    "            predicted_states.append(next_state)\n",
    "            current_state = next_state\n",
    "        \n",
    "        predicted_states = torch.cat(predicted_states, dim=0)\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = compute_mpc_cost(\n",
    "            predicted_states,\n",
    "            goal,\n",
    "            action_sequence,\n",
    "            terminal_weight=terminal_weight\n",
    "        )\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        if verbose and iteration_count[0] % 5 == 0:\n",
    "            print(f\"Iteration {iteration_count[0]:3d}: Cost = {cost.item():.6f}\")\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    # Run optimization\n",
    "    for _ in range(max_iterations):\n",
    "        optimizer.step(closure)\n",
    "        if iteration_count[0] >= max_iterations * 20:  # Total function evals\n",
    "            break\n",
    "    \n",
    "    return action_sequence.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for Running MPC Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mpc_episode(\n",
    "    env,\n",
    "    dynamics_model,\n",
    "    mpc_function,\n",
    "    max_steps=200,\n",
    "    render=False,\n",
    "    verbose=True,\n",
    "    # ===== MPC HYPERPARAMETERS =====\n",
    "    horizon=25,              # âœ… Planning horizon (increased from 12)\n",
    "    num_iterations=150,      # âœ… Optimization iterations (for Adam)\n",
    "    learning_rate=0.01,       # âœ… Learning rate (for Adam)\n",
    "    terminal_weight=10.0,    # âœ… Terminal state weight\n",
    "    max_iterations=20        # âœ… For L-BFGS optimizer\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a full episode using MPC planner.\n",
    "    \n",
    "    Args:\n",
    "        env: Gym environment\n",
    "        dynamics_model: Trained dynamics model\n",
    "        mpc_function: MPC planner function (mpc_planner or mpc_planner_lbfgs)\n",
    "        max_steps: Maximum episode length\n",
    "        render: Whether to render environment\n",
    "        verbose: Print progress\n",
    "        horizon: MPC planning horizon\n",
    "        num_iterations: Number of optimization iterations (Adam)\n",
    "        learning_rate: Learning rate for optimizer (Adam)\n",
    "        terminal_weight: Weight on terminal state cost\n",
    "        max_iterations: Max iterations for L-BFGS\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with episode statistics\n",
    "    \"\"\"\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    trajectory = {\n",
    "        'states': [],\n",
    "        'actions': [],\n",
    "        'rewards': [],\n",
    "        'costs': []\n",
    "    }\n",
    "    \n",
    "    while not (done or truncated) and step < max_steps:\n",
    "        # Convert to torch tensors\n",
    "        state = torch.FloatTensor(obs[\"observation\"])\n",
    "        goal = torch.FloatTensor(obs[\"desired_goal\"])\n",
    "        \n",
    "        # ===== MPC Optimization =====\n",
    "        with torch.no_grad():\n",
    "            dynamics_model.eval()\n",
    "        \n",
    "        # Prepare kwargs based on which optimizer the mpc_function uses\n",
    "        mpc_kwargs = {\n",
    "            'state': state,\n",
    "            'goal': goal,\n",
    "            'action_size': env.action_space.shape[0],\n",
    "            'dynamics_model': dynamics_model,\n",
    "            'horizon': horizon,\n",
    "            'terminal_weight': terminal_weight,\n",
    "            'verbose': (verbose and step % 10 == 0)\n",
    "        }\n",
    "        \n",
    "        # Add optimizer-specific parameters\n",
    "        if 'lbfgs' in mpc_function.__name__.lower():\n",
    "            mpc_kwargs['max_iterations'] = max_iterations\n",
    "        else:  # Adam optimizer\n",
    "            mpc_kwargs['num_iterations'] = num_iterations\n",
    "            mpc_kwargs['learning_rate'] = learning_rate\n",
    "        \n",
    "        action = mpc_function(**mpc_kwargs)\n",
    "        \n",
    "        # Convert to numpy and clip\n",
    "        action_np = action.detach().cpu().numpy()\n",
    "        action_np = np.clip(action_np, env.action_space.low, env.action_space.high)\n",
    "        \n",
    "        # Execute action\n",
    "        obs, reward, done, truncated, info = env.step(action_np)\n",
    "        \n",
    "        # Store trajectory data\n",
    "        trajectory['states'].append(state.numpy())\n",
    "        trajectory['actions'].append(action_np)\n",
    "        trajectory['rewards'].append(reward)\n",
    "        \n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "        \n",
    "        if verbose and step % 20 == 0:\n",
    "            print(f\"Step {step:3d}: Reward = {reward:7.3f}, \"\n",
    "                  f\"Total = {total_reward:7.2f}, \"\n",
    "                  f\"Success = {info.get('is_success', False)}\")\n",
    "    \n",
    "    # Final statistics\n",
    "    # ===== FIX: Compute distance in PHYSICAL space, not scaled space =====\n",
    "    scales = np.array(env.unwrapped.config[\"observation\"][\"scales\"])  # [100, 100, 5, 5, 1, 1]\n",
    "    \n",
    "    # Unscale to get physical values\n",
    "    achieved_physical = obs[\"achieved_goal\"] * scales\n",
    "    desired_physical = obs[\"desired_goal\"] * scales\n",
    "    \n",
    "    # Compute physical distance (position only: x, y)\n",
    "    position_error_physical = np.linalg.norm(achieved_physical[:2] - desired_physical[:2])\n",
    "    \n",
    "    # Full state distance (for debugging)\n",
    "    full_error_physical = np.linalg.norm(achieved_physical - desired_physical)\n",
    "    \n",
    "    stats = {\n",
    "        'episode_length': step,\n",
    "        'total_reward': total_reward,\n",
    "        'success': info.get('is_success', False),\n",
    "        'final_distance': position_error_physical,  # âœ… FIXED: Physical distance in meters\n",
    "        'final_distance_full': full_error_physical,  # All state dimensions\n",
    "        'trajectory': trajectory\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EPISODE SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Steps: {step}\")\n",
    "        print(f\"Total Reward: {total_reward:.2f}\")\n",
    "        print(f\"Success: {stats['success']}\")\n",
    "        print(f\"Final Position Error: {stats['final_distance']:.4f} m\")  # âœ… Physical meters\n",
    "        print(f\"Final State Error (all dims): {stats['final_distance_full']:.4f}\")\n",
    "        \n",
    "        # âœ… NEW: Show detailed breakdown\n",
    "        pos_error = achieved_physical[:2] - desired_physical[:2]\n",
    "        heading_error = np.arctan2(achieved_physical[5], achieved_physical[4]) - \\\n",
    "                       np.arctan2(desired_physical[5], desired_physical[4])\n",
    "        print(f\"  â””â”€ X error: {pos_error[0]:.3f} m\")\n",
    "        print(f\"  â””â”€ Y error: {pos_error[1]:.3f} m\")\n",
    "        print(f\"  â””â”€ Heading error: {np.rad2deg(heading_error):.1f}Â°\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TEST MPC PLANNER ON SINGLE TRANSITION =====\n",
    "parking_config_mpc = {\n",
    "    \"vehicles_count\": 0,\n",
    "    \"reward_weights\": [1, 0.3, 0, 0, 0.2, 0.2],\n",
    "    \"duration\": 100,\n",
    "}\n",
    "env = gym.make(\"parking-v0\", render_mode='rgb_array')\n",
    "obs, info = env.reset()\n",
    "\n",
    "print(\"Testing MPC planner...\")\n",
    "print(f\"Current state: {obs['observation']}\")\n",
    "print(f\"Goal state: {obs['desired_goal']}\")\n",
    "\n",
    "# Test with Adam optimizer\n",
    "action_adam = mpc_planner(\n",
    "    state=torch.FloatTensor(obs[\"observation\"]),\n",
    "    goal=torch.FloatTensor(obs[\"desired_goal\"]),\n",
    "    action_size=env.action_space.shape[0],\n",
    "    dynamics_model=dynamics,\n",
    "    horizon=25,\n",
    "    num_iterations=150,\n",
    "    learning_rate=0.01,\n",
    "    terminal_weight=10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nPlanned action (Adam): {action_adam.numpy()}\")\n",
    "print(f\"Action bounds: [{env.action_space.low}, {env.action_space.high}]\")\n",
    "\n",
    "# ===== DIAGNOSTIC: Show scaling impact =====\n",
    "scales = np.array(env.unwrapped.config[\"observation\"][\"scales\"])\n",
    "scaled_state = obs[\"observation\"]\n",
    "scaled_goal = obs[\"desired_goal\"]\n",
    "physical_state = scaled_state * scales\n",
    "physical_goal = scaled_goal * scales\n",
    "\n",
    "print(f\"\\nðŸ” SCALING DIAGNOSTIC:\")\n",
    "print(f\"  Scaled state:    {scaled_state}\")\n",
    "print(f\"  Physical state:  {physical_state}\")\n",
    "print(f\"  Scaled goal:     {scaled_goal}\")\n",
    "print(f\"  Physical goal:   {physical_goal}\")\n",
    "\n",
    "scaled_error = np.linalg.norm(scaled_state - scaled_goal)\n",
    "physical_error = np.linalg.norm(physical_state[:2] - physical_goal[:2])  # Position only\n",
    "print(f\"\\n  âŒ Error in scaled space: {scaled_error:.4f} (MISLEADING!)\")\n",
    "print(f\"  âœ… Error in physical space (x,y): {physical_error:.4f} m (TRUE!)\")\n",
    "\n",
    "# ===== RUN FULL EPISODE WITH MPC =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RUNNING FULL EPISODE WITH MPC\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "stats = run_mpc_episode(\n",
    "    env=env,\n",
    "    dynamics_model=dynamics,\n",
    "    mpc_function=mpc_planner,\n",
    "    max_steps=200,\n",
    "    render=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ===== COMPARE: Run with CEM for comparison =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: RUNNING WITH CEM PLANNER\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "step = 0\n",
    "total_reward_cem = 0\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "while not (done or truncated) and step < 200:\n",
    "    action_cem = cem_planner(\n",
    "        torch.FloatTensor(obs[\"observation\"]),\n",
    "        torch.FloatTensor(obs[\"desired_goal\"]),\n",
    "        env.action_space.shape[0]\n",
    "    )\n",
    "    \n",
    "    obs, reward, done, truncated, info = env.step(action_cem.numpy())\n",
    "    total_reward_cem += reward\n",
    "    step += 1\n",
    "\n",
    "print(f\"CEM - Steps: {step}, Reward: {total_reward_cem:.2f}, \"\n",
    "      f\"Success: {info.get('is_success', False)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"MPC - Steps: {stats['episode_length']}, \"\n",
    "      f\"Reward: {stats['total_reward']:.2f}, \"\n",
    "      f\"Success: {stats['success']}\")\n",
    "print(f\"CEM - Steps: {step}, \"\n",
    "      f\"Reward: {total_reward_cem:.2f}, \"\n",
    "      f\"Success: {info.get('is_success', False)}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: RUNNING WITH CEM PLANNER\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "step = 0\n",
    "total_reward_cem = 0\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "while not (done or truncated) and step < 200:\n",
    "    action_cem = cem_planner(\n",
    "        torch.FloatTensor(obs[\"observation\"]),\n",
    "        torch.FloatTensor(obs[\"desired_goal\"]),\n",
    "        env.action_space.shape[0]\n",
    "    )\n",
    "    \n",
    "    obs, reward, done, truncated, info = env.step(action_cem.numpy())\n",
    "    total_reward_cem += reward\n",
    "    step += 1\n",
    "\n",
    "print(f\"CEM - Steps: {step}, Reward: {total_reward_cem:.2f}, \"\n",
    "      f\"Success: {info.get('is_success', False)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"MPC - Steps: {stats['episode_length']}, \"\n",
    "      f\"Reward: {stats['total_reward']:.2f}, \"\n",
    "      f\"Success: {stats['success']}\")\n",
    "print(f\"CEM - Steps: {step}, \"\n",
    "      f\"Reward: {total_reward_cem:.2f}, \"\n",
    "      f\"Success: {info.get('is_success', False)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8L6vEPWyea7"
   },
   "source": [
    "## Visualize a few episodes (CEM)\n",
    "En voiture, Simone !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOcOP7Of18T2"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"parking-v0\", render_mode='rgb_array')\n",
    "duration = env.unwrapped.config[\"duration\"]\n",
    "env = record_videos(env)\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Storage for trajectory data\n",
    "trajectory_data = {\n",
    "    'time': [],\n",
    "    'x': [],\n",
    "    'y': [],\n",
    "    'vx': [],\n",
    "    'vy': [],\n",
    "    'heading': [],\n",
    "    'cos_h': [],\n",
    "    'sin_h': [],\n",
    "    'goal_x': [],\n",
    "    'goal_y': [],\n",
    "    'episode': [],\n",
    "    'reward': [],\n",
    "    'action_accel': [],\n",
    "    'action_steer': []\n",
    "}\n",
    "\n",
    "current_episode = 0\n",
    "time_step = 0\n",
    "\n",
    "for step in trange(3 * duration, desc=\"Testing 3 episodes...\"):\n",
    "    action = cem_planner(torch.Tensor(obs[\"observation\"]),\n",
    "                         torch.Tensor(obs[\"desired_goal\"]),\n",
    "                         env.action_space.shape[0])\n",
    "\n",
    "    # Store current state\n",
    "    trajectory_data['time'].append(time_step)\n",
    "    trajectory_data['x'].append(obs[\"observation\"][0])\n",
    "    trajectory_data['y'].append(obs[\"observation\"][1])\n",
    "    trajectory_data['vx'].append(obs[\"observation\"][2])\n",
    "    trajectory_data['vy'].append(obs[\"observation\"][3])\n",
    "    trajectory_data['cos_h'].append(obs[\"observation\"][4])\n",
    "    trajectory_data['sin_h'].append(obs[\"observation\"][5])\n",
    "\n",
    "    # Compute heading angle from cos_h and sin_h\n",
    "    heading_rad = np.arctan2(obs[\"observation\"][5], obs[\"observation\"][4])\n",
    "    trajectory_data['heading'].append(np.rad2deg(heading_rad))\n",
    "    \n",
    "    # Store goal\n",
    "    trajectory_data['goal_x'].append(obs[\"desired_goal\"][0])\n",
    "    trajectory_data['goal_y'].append(obs[\"desired_goal\"][1])\n",
    "    \n",
    "    # Store action\n",
    "    trajectory_data['action_accel'].append(action.numpy()[0])\n",
    "    trajectory_data['action_steer'].append(action.numpy()[1])\n",
    "    \n",
    "    trajectory_data['episode'].append(current_episode)\n",
    "\n",
    "    obs, reward, done, truncated, info = env.step(action.numpy())\n",
    "    trajectory_data['reward'].append(reward)\n",
    "\n",
    "    time_step += 1\n",
    "\n",
    "    if done or truncated:\n",
    "        obs, info = env.reset()\n",
    "        current_episode += 1\n",
    "        time_step = 0\n",
    "env.close()\n",
    "show_videos()\n",
    "\n",
    "# ===== Convert to numpy arrays for plotting =====\n",
    "for key in trajectory_data:\n",
    "    trajectory_data[key] = np.array(trajectory_data[key])\n",
    "\n",
    "print(f\"\\nTotal timesteps recorded: {len(trajectory_data['time'])}\")\n",
    "print(f\"Episodes completed: {current_episode + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPC Planner Video Recording & Trajectory Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MPC PLANNER: VIDEO RECORDING & TRAJECTORY COLLECTION\n",
    "# ============================================================\n",
    "mpc_config = {\n",
    "    \"duration\": 100,\n",
    "    \"vehicles_count\": 0,\n",
    "    \"reward_weights\": [1, 0.8, 0, 0, 1, 1],\n",
    "    \"steering_range\": np.deg2rad(60),\n",
    "    \"vehicles_count\": 24,\n",
    "    \"action\": {\n",
    "            \"type\": \"ContinuousAction\",\n",
    "            \"acceleration_range\": (-2.0, 2.0),\n",
    "            \"speed_range\": (-3, 3),\n",
    "            },\n",
    "}\n",
    "\n",
    "# ===== SETUP ENVIRONMENT WITH VIDEO RECORDING =====\n",
    "env = gym.make(\"parking-v0\", render_mode='rgb_array', config = mpc_config)\n",
    "duration = int(1.5*env.unwrapped.config[\"duration\"])\n",
    "env = record_videos(env)\n",
    "obs, info = env.reset()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸŽ¬ STARTING MPC VIDEO RECORDING SESSION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Recording 3 episodes...\")\n",
    "print(f\"Episode duration: {duration} steps\")\n",
    "print(f\"MPC Parameters:\")\n",
    "print(f\"  - Horizon: 25 steps\")\n",
    "print(f\"  - Iterations: 150\")\n",
    "print(f\"  - Learning rate: 0.8\")\n",
    "print(f\"  - Terminal weight: 10.0\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# ===== STORAGE FOR TRAJECTORY DATA =====\n",
    "trajectory_data = {\n",
    "    'time': [],           # Timestep within episode\n",
    "    'x': [],              # X position (SCALED)\n",
    "    'y': [],              # Y position (SCALED)\n",
    "    'vx': [],             # X velocity (SCALED)\n",
    "    'vy': [],             # Y velocity (SCALED)\n",
    "    'heading': [],        # Heading angle in degrees\n",
    "    'cos_h': [],          # cos(heading)\n",
    "    'sin_h': [],          # sin(heading)\n",
    "    'goal_x': [],         # Goal X position (SCALED)\n",
    "    'goal_y': [],         # Goal Y position (SCALED)\n",
    "    'episode': [],        # Episode number\n",
    "    'reward': [],         # Step reward\n",
    "    'action_accel': [],   # Acceleration action\n",
    "    'action_steer': [],   # Steering action\n",
    "    'mpc_cost': [],       # MPC optimization cost (NEW!)\n",
    "}\n",
    "\n",
    "current_episode = 0\n",
    "time_step = 0\n",
    "total_mpc_time = 0  # Track MPC computation time\n",
    "\n",
    "# ===== MPC PARAMETERS =====\n",
    "# These match your improved configuration from Cell 33\n",
    "MPC_HORIZON = 25\n",
    "MPC_ITERATIONS = 150\n",
    "MPC_LEARNING_RATE = 0.01\n",
    "MPC_TERMINAL_WEIGHT = 10.0\n",
    "\n",
    "# ===== MAIN LOOP: RUN 3 EPISODES =====\n",
    "for step in trange(duration, desc=\"ðŸš— Running MPC episodes\"):\n",
    "    # ===== CALL MPC PLANNER =====\n",
    "    # Convert observations to torch tensors\n",
    "    state_tensor = torch.FloatTensor(obs[\"observation\"])\n",
    "    goal_tensor = torch.FloatTensor(obs[\"desired_goal\"])\n",
    "    \n",
    "    # Plan action using MPC\n",
    "    # Note: verbose=False to avoid cluttering output during recording\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    action_tensor = mpc_planner(\n",
    "        state=state_tensor,\n",
    "        goal=goal_tensor,\n",
    "        action_size=env.action_space.shape[0],\n",
    "        dynamics_model=dynamics,\n",
    "        horizon=MPC_HORIZON,\n",
    "        num_iterations=MPC_ITERATIONS,\n",
    "        learning_rate=MPC_LEARNING_RATE,\n",
    "        terminal_weight=MPC_TERMINAL_WEIGHT,\n",
    "        verbose=False  # âœ… Silent mode for video recording\n",
    "    )\n",
    "    \n",
    "    mpc_time = time.time() - start_time\n",
    "    total_mpc_time += mpc_time\n",
    "    \n",
    "    # Convert to numpy for environment\n",
    "    action = action_tensor.detach().cpu().numpy()\n",
    "    \n",
    "    # ===== STORE CURRENT STATE (SCALED VALUES) =====\n",
    "    trajectory_data['time'].append(time_step)\n",
    "    trajectory_data['x'].append(obs[\"observation\"][0])        # x/100\n",
    "    trajectory_data['y'].append(obs[\"observation\"][1])        # y/100\n",
    "    trajectory_data['vx'].append(obs[\"observation\"][2])       # vx/5\n",
    "    trajectory_data['vy'].append(obs[\"observation\"][3])       # vy/5\n",
    "    trajectory_data['cos_h'].append(obs[\"observation\"][4])    # cos(heading)\n",
    "    trajectory_data['sin_h'].append(obs[\"observation\"][5])    # sin(heading)\n",
    "\n",
    "    # ===== COMPUTE HEADING ANGLE =====\n",
    "    # Convert from (cos_h, sin_h) to degrees\n",
    "    heading_rad = np.arctan2(obs[\"observation\"][5], obs[\"observation\"][4])\n",
    "    trajectory_data['heading'].append(np.rad2deg(heading_rad))\n",
    "    \n",
    "    # ===== STORE GOAL (SCALED VALUES) =====\n",
    "    trajectory_data['goal_x'].append(obs[\"desired_goal\"][0])  # goal_x/100\n",
    "    trajectory_data['goal_y'].append(obs[\"desired_goal\"][1])  # goal_y/100\n",
    "    \n",
    "    # ===== STORE ACTIONS =====\n",
    "    trajectory_data['action_accel'].append(action[0])  # Normalized [-1, 1]\n",
    "    trajectory_data['action_steer'].append(action[1])  # Normalized [-1, 1]\n",
    "    \n",
    "    # ===== STORE EPISODE NUMBER =====\n",
    "    trajectory_data['episode'].append(current_episode)\n",
    "    \n",
    "    # ===== COMPUTE MPC COST FOR THIS STATE =====\n",
    "    # This helps debug MPC performance\n",
    "    with torch.no_grad():\n",
    "        # Predict trajectory from current state\n",
    "        actions_tensor = action_tensor.unsqueeze(0).repeat(MPC_HORIZON, 1)\n",
    "        states_pred = [state_tensor.unsqueeze(0)]\n",
    "        current_pred = state_tensor.unsqueeze(0)\n",
    "        \n",
    "        for t in range(MPC_HORIZON):\n",
    "            next_pred = dynamics(current_pred, actions_tensor[t].unsqueeze(0))\n",
    "            states_pred.append(next_pred)\n",
    "            current_pred = next_pred\n",
    "        \n",
    "        states_pred = torch.cat(states_pred, dim=0)\n",
    "        cost = compute_mpc_cost(states_pred, goal_tensor, actions_tensor, \n",
    "                               terminal_weight=MPC_TERMINAL_WEIGHT)\n",
    "        trajectory_data['mpc_cost'].append(cost.item())\n",
    "\n",
    "    # ===== EXECUTE ACTION IN ENVIRONMENT =====\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    trajectory_data['reward'].append(reward)\n",
    "\n",
    "    time_step += 1\n",
    "\n",
    "    # ===== HANDLE EPISODE TERMINATION =====\n",
    "    if done or truncated:\n",
    "        print(f\"Episode {current_episode} finished:\")\n",
    "        print(f\"  Steps: {time_step}\")\n",
    "        print(f\"  Total reward: {sum([r for i, r in enumerate(trajectory_data['reward']) if trajectory_data['episode'][i] == current_episode]):.2f}\")\n",
    "        print(f\"  Success: {info.get('is_success', False)}\")\n",
    "        print(f\"  Avg MPC time: {total_mpc_time/time_step:.3f}s per step\\n\")\n",
    "        \n",
    "        # Reset for next episode\n",
    "        obs, info = env.reset()\n",
    "        current_episode += 1\n",
    "        time_step = 0\n",
    "        total_mpc_time = 0\n",
    "\n",
    "# ===== CLEANUP =====\n",
    "env.close()\n",
    "print(\"\\nâœ… Video recording complete!\")\n",
    "#print(f\"ðŸ“¹ Videos saved to: {env.unwrapped.video_folder}\")\n",
    "\n",
    "# ===== SHOW VIDEOS =====\n",
    "show_videos()\n",
    "\n",
    "# ===== CONVERT TO NUMPY ARRAYS =====\n",
    "for key in trajectory_data:\n",
    "    trajectory_data[key] = np.array(trajectory_data[key])\n",
    "\n",
    "# ===== PRINT SUMMARY =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š TRAJECTORY DATA SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total timesteps recorded: {len(trajectory_data['time'])}\")\n",
    "print(f\"Episodes completed: {current_episode}\")\n",
    "print(f\"Data keys: {list(trajectory_data.keys())}\")\n",
    "print(f\"Data shape example (x): {trajectory_data['x'].shape}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===== BASIC STATISTICS =====\n",
    "unique_episodes = np.unique(trajectory_data['episode'])\n",
    "print(f\"\\nðŸ“ˆ PER-EPISODE STATISTICS:\")\n",
    "for ep in unique_episodes:\n",
    "    mask = trajectory_data['episode'] == ep\n",
    "    ep_reward = trajectory_data['reward'][mask].sum()\n",
    "    ep_length = mask.sum()\n",
    "    \n",
    "    # Compute final error (in PHYSICAL space)\n",
    "    scales = np.array([100, 100, 5, 5, 1, 1])\n",
    "    final_x = trajectory_data['x'][mask][-1] * scales[0]\n",
    "    final_y = trajectory_data['y'][mask][-1] * scales[1]\n",
    "    goal_x = trajectory_data['goal_x'][mask][-1] * scales[0]\n",
    "    goal_y = trajectory_data['goal_y'][mask][-1] * scales[1]\n",
    "    final_error = np.sqrt((final_x - goal_x)**2 + (final_y - goal_y)**2)\n",
    "    \n",
    "    print(f\"  Episode {int(ep)}:\")\n",
    "    print(f\"    Length: {ep_length} steps\")\n",
    "    print(f\"    Total reward: {ep_reward:.2f}\")\n",
    "    print(f\"    Final position error: {final_error:.3f} m\")\n",
    "    print(f\"    Avg MPC cost: {trajectory_data['mpc_cost'][mask].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PLOT 1: Position Trajectories (X-Y Plot) =====\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('CEM Planner Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Get episode boundaries for coloring\n",
    "episodes = trajectory_data['episode']\n",
    "unique_episodes = np.unique(episodes)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(unique_episodes)))\n",
    "\n",
    "# Plot 1: X-Y Trajectory\n",
    "ax = axes[0, 0]\n",
    "for i, ep in enumerate(unique_episodes):\n",
    "    mask = episodes == ep\n",
    "    ax.plot(trajectory_data['x'][mask], \n",
    "            trajectory_data['y'][mask], \n",
    "            color=colors[i], \n",
    "            label=f'Episode {int(ep)}',\n",
    "            linewidth=2,\n",
    "            marker='o',\n",
    "            markersize=2)\n",
    "    \n",
    "    # Mark start and end\n",
    "    ax.plot(trajectory_data['x'][mask][0], \n",
    "            trajectory_data['y'][mask][0], \n",
    "            'go', markersize=10, label='Start' if i == 0 else '')\n",
    "    ax.plot(trajectory_data['x'][mask][-1], \n",
    "            trajectory_data['y'][mask][-1], \n",
    "            'r*', markersize=15, label='End' if i == 0 else '')\n",
    "    \n",
    "    # Mark goal\n",
    "    ax.plot(trajectory_data['goal_x'][mask][0], \n",
    "            trajectory_data['goal_y'][mask][0], \n",
    "            'rs', markersize=12, label='Goal' if i == 0 else '')\n",
    "\n",
    "ax.set_xlabel('X Position [m]', fontsize=12)\n",
    "ax.set_ylabel('Y Position [m]', fontsize=12)\n",
    "ax.set_title('X-Y Trajectory', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=9)\n",
    "ax.axis('equal')\n",
    "\n",
    "# Plot 2: X Position vs Time\n",
    "ax = axes[0, 1]\n",
    "for i, ep in enumerate(unique_episodes):\n",
    "    mask = episodes == ep\n",
    "    time_ep = np.arange(np.sum(mask))\n",
    "    ax.plot(time_ep, trajectory_data['x'][mask], \n",
    "            color=colors[i], label=f'Episode {int(ep)}', linewidth=2)\n",
    "    ax.axhline(y=trajectory_data['goal_x'][mask][0], \n",
    "               color=colors[i], linestyle='--', alpha=0.5, linewidth=1)\n",
    "\n",
    "ax.set_xlabel('Time [steps]', fontsize=12)\n",
    "ax.set_ylabel('X Position [m]', fontsize=12)\n",
    "ax.set_title('X Position over Time', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# Plot 3: Y Position vs Time\n",
    "ax = axes[0, 2]\n",
    "for i, ep in enumerate(unique_episodes):\n",
    "    mask = episodes == ep\n",
    "    time_ep = np.arange(np.sum(mask))\n",
    "    ax.plot(time_ep, trajectory_data['y'][mask], \n",
    "            color=colors[i], label=f'Episode {int(ep)}', linewidth=2)\n",
    "    ax.axhline(y=trajectory_data['goal_y'][mask][0], \n",
    "               color=colors[i], linestyle='--', alpha=0.5, linewidth=1)\n",
    "\n",
    "ax.set_xlabel('Time [steps]', fontsize=12)\n",
    "ax.set_ylabel('Y Position [m]', fontsize=12)\n",
    "ax.set_title('Y Position over Time', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# Plot 4: Velocity Magnitude\n",
    "ax = axes[1, 0]\n",
    "for i, ep in enumerate(unique_episodes):\n",
    "    mask = episodes == ep\n",
    "    time_ep = np.arange(np.sum(mask))\n",
    "    velocity = np.sqrt(trajectory_data['vx'][mask]**2 + trajectory_data['vy'][mask]**2)\n",
    "    ax.plot(time_ep, velocity, \n",
    "            color=colors[i], label=f'Episode {int(ep)}', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Time [steps]', fontsize=12)\n",
    "ax.set_ylabel('Speed [m/s]', fontsize=12)\n",
    "ax.set_title('Velocity Magnitude over Time', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# Plot 5: Heading Angle\n",
    "ax = axes[1, 1]\n",
    "for i, ep in enumerate(unique_episodes):\n",
    "    mask = episodes == ep\n",
    "    time_ep = np.arange(np.sum(mask))\n",
    "    ax.plot(time_ep, trajectory_data['heading'][mask], \n",
    "            color=colors[i], label=f'Episode {int(ep)}', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Time [steps]', fontsize=12)\n",
    "ax.set_ylabel('Heading [degrees]', fontsize=12)\n",
    "ax.set_title('Heading Angle over Time', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# Plot 6: Actions\n",
    "ax = axes[1, 2]\n",
    "for i, ep in enumerate(unique_episodes):\n",
    "    mask = episodes == ep\n",
    "    time_ep = np.arange(np.sum(mask))\n",
    "    ax.plot(time_ep, trajectory_data['action_accel'][mask], \n",
    "            color=colors[i], linestyle='-', linewidth=2, \n",
    "            label=f'Ep {int(ep)} Accel' if i < 2 else '')\n",
    "    ax.plot(time_ep, trajectory_data['action_steer'][mask], \n",
    "            color=colors[i], linestyle='--', linewidth=2, \n",
    "            label=f'Ep {int(ep)} Steer' if i < 2 else '')\n",
    "\n",
    "ax.set_xlabel('Time [steps]', fontsize=12)\n",
    "ax.set_ylabel('Action Value', fontsize=12)\n",
    "ax.set_title('Control Actions', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=9)\n",
    "ax.axhline(y=0, color='k', linestyle='-', alpha=0.3, linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PLOT EACH EPISODE SEPARATELY =====\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get episode boundaries\n",
    "episodes = trajectory_data['episode']\n",
    "unique_episodes = np.unique(episodes)\n",
    "n_episodes = len(unique_episodes)\n",
    "\n",
    "# Create a figure for each episode\n",
    "for ep_idx, ep in enumerate(unique_episodes):\n",
    "    mask = episodes == ep\n",
    "    \n",
    "    # Create subplots for this episode\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle(f'Episode {int(ep)} - MBRL Performance', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Get data for this episode\n",
    "    x = 100*trajectory_data['x'][mask]\n",
    "    y = 100*trajectory_data['y'][mask]\n",
    "    vx = 2*trajectory_data['vx'][mask]\n",
    "    vy = 2*trajectory_data['vy'][mask]\n",
    "    heading = trajectory_data['heading'][mask]\n",
    "    goal_x = 100*trajectory_data['goal_x'][mask][0]\n",
    "    goal_y = 100*trajectory_data['goal_y'][mask][0]\n",
    "    action_accel = trajectory_data['action_accel'][mask]\n",
    "    action_steer = trajectory_data['action_steer'][mask]\n",
    "    time_steps = np.arange(len(x))\n",
    "    \n",
    "    # Plot 1: X-Y Trajectory\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(x, -y, 'b-', linewidth=2, marker='o', markersize=3, label='Trajectory')\n",
    "    ax.plot(x[0], -y[0], 'go', markersize=12, label='Start', zorder=5)\n",
    "    ax.plot(x[-1], -y[-1], 'r*', markersize=18, label='End', zorder=5)\n",
    "    ax.plot(goal_x, -goal_y, 'rs', markersize=15, label='Goal', zorder=5)\n",
    "    \n",
    "    # Add distance annotations\n",
    "    # start_dist = np.sqrt((x[0] - goal_x)**2 + (y[0] - goal_y)**2)\n",
    "    # end_dist = np.sqrt((x[-1] - goal_x)**2 + (y[-1] - goal_y)**2)\n",
    "    # ax.text(0.02, 0.98, f'Initial dist: {start_dist:.2f}m\\nFinal dist: {end_dist:.2f}m',\n",
    "    #         transform=ax.transAxes, verticalalignment='top',\n",
    "    #         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    ax.set_xlabel('X Position [m]', fontsize=12)\n",
    "    ax.set_ylabel('Y Position [m]', fontsize=12)\n",
    "    ax.set_title('X-Y Trajectory', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    # Plot 2: X Position vs Time\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(time_steps, x, 'b-', linewidth=2, label='X Position')\n",
    "    ax.axhline(y=goal_x, color='r', linestyle='--', alpha=0.7, \n",
    "               linewidth=2, label='Goal X')\n",
    "    ax.fill_between(time_steps, goal_x - 0.5, goal_x + 0.5, \n",
    "                     alpha=0.2, color='green', label='Goal tolerance')\n",
    "    ax.set_xlabel('Time [steps]', fontsize=12)\n",
    "    ax.set_ylabel('X Position [m]', fontsize=12)\n",
    "    ax.set_title('X Position over Time', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=10)\n",
    "    \n",
    "    # Plot 3: Y Position vs Time\n",
    "    ax = axes[0, 2]\n",
    "    ax.plot(time_steps, y, 'b-', linewidth=2, label='Y Position')\n",
    "    ax.axhline(y=goal_y, color='r', linestyle='--', alpha=0.7, \n",
    "               linewidth=2, label='Goal Y')\n",
    "    ax.fill_between(time_steps, goal_y - 0.5, goal_y + 0.5, \n",
    "                     alpha=0.2, color='green', label='Goal tolerance')\n",
    "    ax.set_xlabel('Time [steps]', fontsize=12)\n",
    "    ax.set_ylabel('Y Position [m]', fontsize=12)\n",
    "    ax.set_title('Y Position over Time', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=10)\n",
    "    \n",
    "    # Plot 4: Velocity Components\n",
    "    ax = axes[1, 0]\n",
    "    velocity_mag = np.sqrt(vx**2 + vy**2)\n",
    "    ax.plot(time_steps, velocity_mag, 'b-', linewidth=2, label='Speed')\n",
    "    ax.plot(time_steps, -vx, 'g--', linewidth=1.5, alpha=0.7, label='Vx')\n",
    "    ax.plot(time_steps, -vy, 'r--', linewidth=1.5, alpha=0.7, label='Vy')\n",
    "    ax.axhline(y=0, color='k', linestyle='-', alpha=0.2, linewidth=0.5)\n",
    "    ax.set_xlabel('Time [steps]', fontsize=12)\n",
    "    ax.set_ylabel('Velocity [m/s]', fontsize=12)\n",
    "    ax.set_title('Velocity over Time', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=10)\n",
    "    \n",
    "    # Plot 5: Heading Angle\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(time_steps, heading, 'b-', linewidth=2, label='Heading')\n",
    "    ax.axhline(y=90, color='k', linestyle='-', linewidth=1)\n",
    "    ax.set_xlabel('Time [steps]', fontsize=12)\n",
    "    ax.set_ylabel('Heading [degrees]', fontsize=12)\n",
    "    ax.set_title('Heading Angle over Time', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=10)\n",
    "    \n",
    "    # Plot 6: Control Actions\n",
    "    ax = axes[1, 2]\n",
    "    ax.plot(time_steps, -action_accel, 'b-', linewidth=2, label='Acceleration')\n",
    "    ax.plot(time_steps, action_steer, 'r-', linewidth=2, label='Steering')\n",
    "    ax.axhline(y=0, color='k', linestyle='-', alpha=0.2, linewidth=0.5)\n",
    "    ax.set_xlabel('Time [steps]', fontsize=12)\n",
    "    ax.set_ylabel('Action Value', fontsize=12)\n",
    "    ax.set_title('Control Actions', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=10)\n",
    "    \n",
    "    # # Add summary statistics\n",
    "    # stats_text = f'Steps: {len(x)}\\n'\n",
    "    # stats_text += f'Avg Speed: {np.mean(velocity_mag):.2f} m/s\\n'\n",
    "    # stats_text += f'Max Speed: {np.max(velocity_mag):.2f} m/s\\n'\n",
    "    # stats_text += f'Final Error: {end_dist:.2f} m'\n",
    "    # ax.text(0.02, 0.98, stats_text,\n",
    "    #         transform=ax.transAxes, verticalalignment='top',\n",
    "    #         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5),\n",
    "    #         fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Episode {int(ep)} Summary:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Duration: {len(x)} steps\")\n",
    "    # print(f\"  Initial distance to goal: {start_dist:.3f} m\")\n",
    "    # print(f\"  Final distance to goal: {end_dist:.3f} m\")\n",
    "    # print(f\"  Distance improvement: {start_dist - end_dist:.3f} m\")\n",
    "    # print(f\"  Average speed: {np.mean(velocity_mag):.3f} m/s\")\n",
    "    # print(f\"  Max speed: {np.max(velocity_mag):.3f} m/s\")\n",
    "    # print(f\"  Success: {'âœ… Yes' if end_dist < 0.5 else 'âŒ No'}\")\n",
    "    # print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Create a mask for episode 2\n",
    "episode_mask = trajectory_data['episode'] == 1\n",
    "\n",
    "# Filter all arrays in the dictionary using the mask\n",
    "trajectory_data_episode_2 = {key: values[episode_mask] for key, values in trajectory_data.items()}\n",
    "\n",
    "# Save the trajectory data for episode 2\n",
    "with open('trajectory_data_episode_1_mpcRevParkEmpty.pkl', 'wb') as f:\n",
    "    pickle.dump(trajectory_data_episode_2, f)\n",
    "\n",
    "print(f\"âœ… Saved episode 2 data with {len(trajectory_data_episode_2['time'])} timesteps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lOad the episode 2 data and plot the trajectories\n",
    "with open('trajectory_data_episode_1_mpcRevParkEmpty.pkl', 'rb') as f:\n",
    "    trajectory_data_episode_2 = pickle.load(f)\n",
    "\n",
    "# Plot the trajectories\n",
    "trajectory_data_episode_2['x'] = trajectory_data_episode_2['x'] * 100\n",
    "trajectory_data_episode_2['y'] = trajectory_data_episode_2['y'] * 100\n",
    "\n",
    "# Plot the trajectories\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.plot(trajectory_data_episode_2['x'], trajectory_data_episode_2['y'], 'b-', linewidth=2)\n",
    "ax.set_xlabel('X Position [m]')\n",
    "ax.set_ylabel('Y Position [m]')\n",
    "ax.set_title('Trajectory')\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Detailed Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PLOT 2: Velocity Components =====\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Velocity Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Vx over time\n",
    "ax = axes[0, 0]\n",
    "for i, ep in enumerate(unique_episodes):\n",
    "    mask = episodes == ep\n",
    "    time_ep = np.arange(np.sum(mask))\n",
    "    ax.plot(time_ep, trajectory_data['vx'][mask], \n",
    "            color=colors[i], label=f'Episode {int(ep)}', linewidth=2)\n",
    "ax.set_xlabel('Time [steps]')\n",
    "ax.set_ylabel('Vx [m/s]')\n",
    "ax.set_title('X-Velocity')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Vy over time\n",
    "ax = axes[0, 1]\n",
    "for i, ep in enumerate(unique_episodes):\n",
    "    mask = episodes == ep\n",
    "    time_ep = np.arange(np.sum(mask))\n",
    "    ax.plot(time_ep, trajectory_data['vy'][mask], \n",
    "            color=colors[i], label=f'Episode {int(ep)}', linewidth=2)\n",
    "ax.set_xlabel('Time [steps]')\n",
    "ax.set_ylabel('Vy [m/s]')\n",
    "ax.set_title('Y-Velocity')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Velocity phase plot (vx vs vy)\n",
    "ax = axes[1, 0]\n",
    "for i, ep in enumerate(unique_episodes):\n",
    "    mask = episodes == ep\n",
    "    ax.plot(trajectory_data['vx'][mask], \n",
    "            trajectory_data['vy'][mask], \n",
    "            color=colors[i], label=f'Episode {int(ep)}', \n",
    "            linewidth=2, marker='o', markersize=2)\n",
    "ax.set_xlabel('Vx [m/s]')\n",
    "ax.set_ylabel('Vy [m/s]')\n",
    "ax.set_title('Velocity Phase Plot')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "ax.axvline(x=0, color='k', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Reward over time\n",
    "ax = axes[1, 1]\n",
    "for i, ep in enumerate(unique_episodes):\n",
    "    mask = episodes == ep\n",
    "    time_ep = np.arange(np.sum(mask))\n",
    "    ax.plot(time_ep, trajectory_data['reward'][mask], \n",
    "            color=colors[i], label=f'Episode {int(ep)}', linewidth=2)\n",
    "ax.set_xlabel('Time [steps]')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Reward Signal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== COMPUTE METRICS PER EPISODE =====\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMANCE METRICS PER EPISODE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for ep in unique_episodes:\n",
    "    mask = episodes == ep\n",
    "    \n",
    "    # Final position error\n",
    "    final_x = trajectory_data['x'][mask][-1]\n",
    "    final_y = trajectory_data['y'][mask][-1]\n",
    "    goal_x = trajectory_data['goal_x'][mask][0]\n",
    "    goal_y = trajectory_data['goal_y'][mask][0]\n",
    "    \n",
    "    pos_error = np.sqrt((final_x - goal_x)**2 + (final_y - goal_y)**2)\n",
    "    \n",
    "    # Final heading error\n",
    "    final_heading = trajectory_data['heading'][mask][-1]\n",
    "    goal_heading = np.rad2deg(np.arctan2(\n",
    "        trajectory_data['goal_y'][mask][0] - trajectory_data['y'][mask][0],\n",
    "        trajectory_data['goal_x'][mask][0] - trajectory_data['x'][mask][0]\n",
    "    ))\n",
    "    heading_error = abs(final_heading - goal_heading)\n",
    "    \n",
    "    # Average reward\n",
    "    avg_reward = np.mean(trajectory_data['reward'][mask])\n",
    "    total_reward = np.sum(trajectory_data['reward'][mask])\n",
    "    \n",
    "    # Max velocity\n",
    "    max_speed = np.max(np.sqrt(trajectory_data['vx'][mask]**2 + \n",
    "                                trajectory_data['vy'][mask]**2))\n",
    "    \n",
    "    # Episode duration\n",
    "    episode_length = np.sum(mask)\n",
    "    \n",
    "    print(f\"\\nEpisode {int(ep)}:\")\n",
    "    #Print goal position\n",
    "    print(f\"  Goal Position:         {goal_x:.3f}, {goal_y:.3f}\")\n",
    "    print(f\"  Final Position Error:  {pos_error:.3f} m\")\n",
    "    print(f\"  Final Heading Error:   {heading_error:.1f}Â°\")\n",
    "    print(f\"  Average Reward:        {avg_reward:.3f}\")\n",
    "    print(f\"  Total Reward:          {total_reward:.2f}\")\n",
    "    print(f\"  Max Speed:             {max_speed:.2f} m/s\")\n",
    "    print(f\"  Episode Length:        {episode_length} steps\")\n",
    "    print(f\"  Success: {'âœ… YES' if pos_error < 0.5 else 'âŒ NO'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psBBQIv4fvjT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Limitations\n",
    "\n",
    "### Model bias\n",
    "\n",
    "In model-based reinforcement learning, we replace our original optimal control problem by another problem: optimizing our learnt approximate MDP. When settling for this approximate MDP to plan with, we introduce a **bias** that can only **decrease the true performance** of the corresponding planned policy. This is called the problem of model bias.\n",
    "\n",
    "In some MDPs, even slight model errors lead to a dramatic drop in performance, as illustrated in the beginning of the following video:\n",
    "\n",
    "[![Approximate Robust Control of Uncertain Dynamical Systems](https://img.youtube.com/vi/8khqd3BJo0A/0.jpg)](https://www.youtube.com/watch?v=8khqd3BJo0A)\n",
    "\n",
    "The question of how to address model bias belongs to the field of **Safe Reinforcement Learning**. \n",
    "\n",
    "### [The call of the void](https://www.urbandictionary.com/define.php?term=the%20call%20of%20the%20void)\n",
    "\n",
    "The model will be accurate only on some region of the state space that was explored and covered in $D$.\n",
    "Outside of $D$, the model may diverge and **hallucinate** important rewards.\n",
    "This effect is problematic when the model is used by a planning algorithm, as the latter will try to **exploit** these hallucinated high rewards and will steer the agent towards **unknown** (and thus dangerous) **regions** where the model is erroneously optimistic.\n",
    "\n",
    "### Computational cost of planning\n",
    "\n",
    "At test time, the planning step typically requires **sampling a lot of trajectories** to find a near-optimal candidate, which may turn out to be very costly. This may be prohibitive in a high-frequency real-time setting. The **model-free** methods which directly recommend the best action are **much more efficient** in that regard."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "parking_model_based.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
